import torch
import whisper
from pathlib import Path
import argparse  
import json 
import dataclasses
from dataclasses import dataclass

@dataclass
class Data:
  audio: str
  language: str

device = torch.device("cuda:0")

model = whisper.load_model("small").to(device)

def audio2language(audio_path: str):
    audio = whisper.load_audio(audio_path)
    audio = whisper.pad_or_trim(audio)

    # make log-Mel spectrogram and move to the same device as the model
    mel = whisper.log_mel_spectrogram(audio).to(model.device)

    # detect the spoken language
    _, probs = model.detect_language(mel)
    # print(f"Detected language: {max(probs, key=probs.get)}")
    return max(probs, key=probs.get)
    
if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Get embedding from audio file",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )

    parser.add_argument("--begin_id", type=int, help= \
        "id of starting line in manifest file from voxceleb1 dataset that generated by UnitExtractor")

    parser.add_argument("--end_id", type=int, help= \
        "id of ending line in manifest file from voxceleb1 dataset that generated by UnitExtractor")  

    parser.add_argument("--manifest_path", type=str, default="/content/drive/MyDrive/dev_metadata.json", help="path to manifest file") 
    
    args = parser.parse_args()

    
    with open(args.manifest_path, "r") as f:
        manifest = f.readlines()
    
    for i in range(args.begin_id, args.end_id+1):
        print(i)
        
        line = manifest[i]
        metadata = json.loads(line)
        
        language = audio2language(metadata["audio"])
        dictionary = Data(metadata["audio"], language)
        with open("/content/drive/MyDrive/audio2lang.json", "a") as outfile:
            outfile.write(json.dumps(dataclasses.asdict(dictionary)) + "\n")
            
    print("Xong!")